"""
MJHQ-30K FID and CLIP Score evaluation for VILA-U model
Adapted from HART implementation for vila-u image generation evaluation
"""
import argparse
import json
import os
import random
import time
from pathlib import Path

import clip
import numpy as np
import torch
import torchvision.transforms as transforms
from PIL import Image
from tqdm import tqdm
from cleanfid import fid

import vila_u
from utils import tracker


def compute_clip_score(model, preprocess, image_path, text, device):
    """
    Compute CLIP score between image and text
    """
    # Load and preprocess the image
    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)

    # Tokenize the text
    text = clip.tokenize([text], truncate=True).to(device)

    # Compute the feature vectors
    with torch.no_grad():
        image_features = model.encode_image(image)
        text_features = model.encode_text(text)

    # Normalize the feature vectors
    image_features /= image_features.norm(dim=-1, keepdim=True)
    text_features /= text_features.norm(dim=-1, keepdim=True)

    # Compute the cosine similarity
    similarity = (image_features @ text_features.T).item()
    return similarity


def save_images_vila_u(sample_imgs, output_dir, img_id, category):
    """
    Save images generated by VILA-U model
    """
    os.makedirs(os.path.join(output_dir, category), exist_ok=True)
    
    # VILA-U returns images in format [B, C, H, W], values in [0, 255]
    if isinstance(sample_imgs, torch.Tensor):
        sample_imgs_np = sample_imgs.cpu().numpy()
    else:
        sample_imgs_np = sample_imgs
    
    num_imgs = sample_imgs_np.shape[0]
    
    for img_idx in range(num_imgs):
        cur_img = sample_imgs_np[img_idx]
        
        # Convert from [C, H, W] to [H, W, C] and ensure uint8
        if cur_img.shape[0] == 3:  # If channels first
            cur_img = cur_img.transpose(1, 2, 0)
        
        # Ensure values are in [0, 255] range
        if cur_img.max() <= 1.0:
            cur_img = (cur_img * 255).astype(np.uint8)
        else:
            cur_img = cur_img.astype(np.uint8)
        
        # Save image
        cur_img_pil = Image.fromarray(cur_img)
        if num_imgs > 1:
            save_path = os.path.join(output_dir, category, f"{img_id}_{img_idx}.png")
        else:
            save_path = os.path.join(output_dir, category, f"{img_id}.png")
        cur_img_pil.save(save_path)


def load_mjhq_metadata(metadata_path):
    """
    Load MJHQ-30K metadata
    """
    with open(metadata_path, 'r') as f:
        meta_data = json.load(f)
    return meta_data


def filter_by_category(meta_data, category_filter=None):
    """
    Filter metadata by category
    """
    if category_filter is None:
        return meta_data
    
    filtered_data = {}
    for img_id, data in meta_data.items():
        if category_filter in data.get('category', []):
            filtered_data[img_id] = data
    
    return filtered_data


def generate_images_vila_u(model, meta_data, output_dir, cfg_scale=3.0, generation_nums=1, max_samples=None):
    """
    Generate images using VILA-U model for MJHQ-30K prompts
    """
    print(f"Generating images with VILA-U model...")
    
    os.makedirs(output_dir, exist_ok=True)
    
    items = list(meta_data.items())
    if max_samples:
        items = items[:max_samples]
    
    for img_id, data in tqdm(items, desc="Generating MJHQ images"):
        prompt = data['prompt']
        category = data.get('category', ['general'])[0] if isinstance(data.get('category'), list) else data.get('category', 'general')
        
        try:
            # Generate image using VILA-U
            response = model.generate_image_content(prompt, cfg_scale, generation_nums)
            
            # Save generated images
            save_images_vila_u(response, output_dir, img_id, category)
        
        except Exception as e:
            print(f"Error generating image for {img_id}: {e}")
            continue


def compute_fid_score(real_dir, generated_dir, device='cuda'):
    """
    Compute FID score between real and generated images
    """
    print(f"Computing FID score...")
    print(f"Real images: {real_dir}")
    print(f"Generated images: {generated_dir}")
    
    try:
        fid_score = fid.compute_fid(real_dir, generated_dir, device=device)
        return fid_score
    except Exception as e:
        print(f"Error computing FID: {e}")
        return None


def compute_clip_scores(generated_dir, meta_data, device='cuda', clip_model='ViT-L/14'):
    """
    Compute CLIP scores for generated images
    """
    print(f"Computing CLIP scores...")
    
    # Load the CLIP model
    model, preprocess = clip.load(clip_model, device=device)
    
    total_score = 0
    count = 0
    scores = []
    
    for root, _, files in os.walk(generated_dir):
        for file in files:
            if file.lower().endswith(('.png', '.jpg', '.jpeg')):
                image_path = os.path.join(root, file)
                
                # Extract image ID from filename
                image_id = os.path.splitext(file)[0]
                # Handle case where filename has suffix like "_0"
                if '_' in image_id:
                    image_id = image_id.split('_')[0]
                
                if image_id in meta_data:
                    prompt = meta_data[image_id]["prompt"]
                    try:
                        score = compute_clip_score(model, preprocess, image_path, prompt, device)
                        total_score += score
                        count += 1
                        scores.append(score)
                    except Exception as e:
                        print(f"Error computing CLIP score for {image_path}: {e}")
                else:
                    print(f"No prompt found for image {image_id}")
    
    if count > 0:
        average_clip_score = total_score / count
        std_clip_score = np.std(scores)
        return {
            'mean': average_clip_score,
            'std': std_clip_score,
            'count': count,
            'scores': scores
        }
    else:
        return None


def main():
    parser = argparse.ArgumentParser(description='MJHQ-30K FID and CLIP evaluation for VILA-U')
    
    # Model paths
    parser.add_argument('--model_path', type=str, required=True,
                        help='Path to VILA-U model')
    
    # Dataset paths
    parser.add_argument('--mjhq_metadata_path', type=str, required=True,
                        help='Path to MJHQ-30K metadata JSON file')
    parser.add_argument('--mjhq_images_path', type=str, required=True,
                        help='Path to MJHQ-30K images directory')
    
    # Generation settings
    parser.add_argument('--cfg_scale', type=float, default=3.0,
                        help='CFG scale for VILA-U image generation')
    parser.add_argument('--generation_nums', type=int, default=1,
                        help='Number of images to generate per prompt')
    parser.add_argument('--seed', type=int, default=42,
                        help='Random seed')
    
    # Evaluation settings
    parser.add_argument('--category_filter', type=str, default='people',
                        help='Category to filter (e.g., people, animals, objects)')
    parser.add_argument('--max_samples', type=int, default=None,
                        help='Maximum number of samples to evaluate')
    parser.add_argument('--clip_model', type=str, default='ViT-L/14',
                        help='CLIP model to use')
    
    # Output settings
    parser.add_argument('--output_dir', type=str, default='./mjhq_vila_u_results',
                        help='Output directory for generated images and results')
    parser.add_argument('--sample_folder_dir', type=str, default=None,
                        help='Directory for saving generated images (legacy compatibility)')
    
    # Technical settings
    parser.add_argument('--device', type=str, default='cuda',
                        help='Device to use for computation')
    parser.add_argument('--batch_size', type=int, default=4,
                        help='Batch size for generation')
    
    # Modes
    parser.add_argument('--generate_only', action='store_true',
                        help='Only generate images, skip evaluation')
    parser.add_argument('--evaluate_only', action='store_true',
                        help='Only evaluate existing images, skip generation')
    parser.add_argument('--fid_only', action='store_true',
                        help='Only compute FID score')
    parser.add_argument('--clip_only', action='store_true',
                        help='Only compute CLIP score')
    
    # Experiment tracking
    parser.add_argument('--exp_name', type=str, default='vila_u_mjhq_evaluation',
                        help='Experiment name')
    parser.add_argument('--report_to', type=str, default='none',
                        help='Where to report results (wandb or none)')
    parser.add_argument('--tracker_project_name', type=str, default='vila-u-evaluation',
                        help='Project name for tracking')
    parser.add_argument('--log_results', action='store_true',
                        help='Log results to tracker')

    args = parser.parse_args()
    
    # Set random seed
    if args.seed is not None:
        random.seed(args.seed)
        np.random.seed(args.seed)
        torch.manual_seed(args.seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(args.seed)
    
    # Set up output directory
    if args.sample_folder_dir:
        # Legacy compatibility
        args.output_dir = args.sample_folder_dir
    
    # Load MJHQ metadata
    print(f"Loading MJHQ metadata from {args.mjhq_metadata_path}")
    meta_data = load_mjhq_metadata(args.mjhq_metadata_path)
    
    # Filter by category if specified
    if args.category_filter:
        print(f"Filtering by category: {args.category_filter}")
        meta_data = filter_by_category(meta_data, args.category_filter)
    
    print(f"Loaded {len(meta_data)} samples")
    
    # Limit samples if specified
    if args.max_samples and args.max_samples < len(meta_data):
        items = list(meta_data.items())[:args.max_samples]
        meta_data = dict(items)
        print(f"Limited to {len(meta_data)} samples")
    
    results = {}
    
    # Generation phase
    if not args.evaluate_only:
        print(f"Loading VILA-U model from {args.model_path}")
        model = vila_u.load(args.model_path)
        
        # Generate images
        generated_dir = os.path.join(args.output_dir, 'generated')
        generate_images_vila_u(
            model, meta_data, generated_dir, 
            args.cfg_scale, args.generation_nums, args.max_samples
        )
        
        if args.generate_only:
            print(f"Image generation completed. Images saved to {generated_dir}")
            return
    else:
        generated_dir = os.path.join(args.output_dir, 'generated')
        if not os.path.exists(generated_dir):
            print(f"Error: Generated images directory not found: {generated_dir}")
            return
    
    # Evaluation phase
    if not args.clip_only:
        # Compute FID score
        real_dir = os.path.join(args.mjhq_images_path, args.category_filter or 'people')
        fid_score = compute_fid_score(real_dir, generated_dir, args.device)
        
        if fid_score is not None:
            results['fid_score'] = fid_score
            print(f"FID Score: {fid_score:.4f}")
        else:
            print("Failed to compute FID score")
    
    if not args.fid_only:
        # Compute CLIP scores
        clip_results = compute_clip_scores(generated_dir, meta_data, args.device, args.clip_model)
        
        if clip_results:
            results['clip_score'] = clip_results['mean']
            results['clip_score_std'] = clip_results['std']
            results['clip_samples'] = clip_results['count']
            print(f"Average CLIP Score: {clip_results['mean']:.4f} Â± {clip_results['std']:.4f}")
            print(f"Total images evaluated: {clip_results['count']}")
        else:
            print("Failed to compute CLIP scores")
    
    # Save results
    results['config'] = {
        'model_path': args.model_path,
        'category_filter': args.category_filter,
        'max_samples': args.max_samples,
        'cfg_scale': args.cfg_scale,
        'generation_nums': args.generation_nums,
        'seed': args.seed,
        'clip_model': args.clip_model
    }
    results['num_samples'] = len(meta_data)
    
    output_file = os.path.join(args.output_dir, 'mjhq_results.json')
    os.makedirs(args.output_dir, exist_ok=True)
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"Results saved to {output_file}")
    
    # Log to tracker if specified
    if args.log_results and args.report_to == 'wandb':
        if 'fid_score' in results:
            fid_dict = {f"{args.exp_name}_fid": results['fid_score']}
            tracker(args, fid_dict, label="", pattern="epoch_step", metric="FID")
        
        if 'clip_score' in results:
            clip_dict = {f"{args.exp_name}_clip": results['clip_score']}
            tracker(args, clip_dict, label="", pattern="epoch_step", metric="CLIP_Score")
    
    print("Evaluation completed!")


if __name__ == '__main__':
    main()